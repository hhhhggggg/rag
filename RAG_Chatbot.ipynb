{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Chatbot (Pinecone-only, OpenAI LLM)\n",
        "\n",
        "이 노트북은 웹 없이도 로컬에서 챗봇 동작을 확인할 수 있는 전용 런타임입니다.\n",
        "\n",
        "## ⚠️ 중요: 실행 전 필수 확인사항\n",
        "1. **커널 재시작**: `Ctrl+Shift+P` → \"Jupyter: Restart Kernel\" 실행\n",
        "2. **순차 실행**: Cell 1 → Cell 2 → Cell 3 → Cell 4 → Cell 5 순서대로 실행\n",
        "3. **중복 실행 금지**: Cell 5를 여러 번 실행하지 마세요 (while 루프가 중복 실행됨)\n",
        "\n",
        "실행 순서: 1) 환경/설치 → 2) Pinecone 연결 → 3) 검색 함수 → 4) LLM 챗봇"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 환경/설치\n",
        "import sys, platform\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "\n",
        "!{sys.executable} -m pip install -q --upgrade pip\n",
        "!{sys.executable} -m pip install -q \"pinecone>=5.0.0\" sentence-transformers rank-bm25 pyyaml openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pinecone 연결\n",
        "import os\n",
        "from pinecone import Pinecone\n",
        "from config import PINECONE_API_KEY, PINECONE_INDEX_NAME\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(PINECONE_INDEX_NAME)\n",
        "print(index.describe_index_stats())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 검색 함수 (Cell 26 요약)\n",
        "import re, numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from config import EMBEDDING_MODEL_NAME\n",
        "\n",
        "model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=\"cpu\")\n",
        "\n",
        "def simple_tokenize(s: str):\n",
        "    return re.findall(r\"[A-Za-z0-9가-힣]+\", (s or \"\").lower())\n",
        "\n",
        "def vector_search(query: str, top_k: int = 50, meta_filter=None):\n",
        "    q_vec = model.encode([f\"query: {query}\"], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
        "    kwargs = {\"vector\": q_vec.tolist(), \"top_k\": top_k, \"include_values\": False, \"include_metadata\": True}\n",
        "    if meta_filter:\n",
        "        kwargs[\"filter\"] = meta_filter\n",
        "    res = index.query(**kwargs)\n",
        "    return [(m[\"id\"], float(m[\"score\"]), m.get(\"metadata\", {})) for m in res.get(\"matches\", [])]\n",
        "\n",
        "def bm25_over_candidates(query: str, candidates):\n",
        "    ids, docs = [], []\n",
        "    for cid, _, meta in candidates:\n",
        "        text = (meta or {}).get(\"text_content\") or \"\"\n",
        "        if not text:\n",
        "            title = (meta or {}).get(\"title\") or \"\"\n",
        "            keywords = (meta or {}).get(\"keywords\") or \"\"\n",
        "            text = f\"{title}\\n{keywords}\"\n",
        "        ids.append(cid)\n",
        "        docs.append(simple_tokenize(text))\n",
        "    if not docs:\n",
        "        return {}\n",
        "    bm25 = BM25Okapi(docs)\n",
        "    scores = bm25.get_scores(simple_tokenize(query)) if query else np.zeros(len(ids))\n",
        "    max_b = float(np.max(scores)) if len(scores) else 0.0\n",
        "    return {ids[i]: (float(scores[i])/max_b if max_b>0 else 0.0) for i in range(len(ids))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 개선된 LLM 챗봇\n",
        "from collections import deque\n",
        "from typing import List, Dict, Tuple\n",
        "from openai import OpenAI\n",
        "from config import OPENAI_API_KEY, LLM_MODEL_NAME, DEFAULT_VECTOR_WEIGHT, DEFAULT_BM25_WEIGHT, DEFAULT_TOP_K, DEFAULT_CONTEXT_CHARS, DEFAULT_CONTEXT_TOP_N\n",
        "import os\n",
        "\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# 개선된 시스템 프롬프트\n",
        "SYSTEM_PROMPT = (\n",
        "    \"너는 RAG 기반 도우미야. 제공된 컨텍스트를 우선 활용해서 간결하고 정확하게 답해.\\n\"\n",
        "    \"근거가 없으면 솔직히 모른다고 말해.\\n\"\n",
        "    \"출처를 bullet로 함께 제공해.\"\n",
        "    )\n",
        "\n",
        "def build_context_improved(query: str, candidates: List[Tuple[str, float, Dict]], vec_w: float, bm25_w: float, top_n: int, max_chars: int):\n",
        "    \"\"\"개선된 컨텍스트 빌딩\"\"\"\n",
        "    bm25_scores = bm25_over_candidates(query, candidates)\n",
        "    scored = []\n",
        "    for cid, v_score, meta in candidates:\n",
        "        b_score = bm25_scores.get(cid, 0.0)\n",
        "        combo = vec_w * float(v_score) + bm25_w * float(b_score)\n",
        "        scored.append((combo, cid, meta))\n",
        "    scored.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    picked, used = [], 0\n",
        "    for _, cid, meta in scored[: max(1, int(top_n) * 3)]:\n",
        "        text = (meta or {}).get(\"text_content\") or (meta or {}).get(\"title\") or \"\"\n",
        "        if not text:\n",
        "            continue\n",
        "        if used + len(text) > max_chars:\n",
        "            continue\n",
        "        picked.append({\n",
        "            \"id\": cid,\n",
        "            \"title\": (meta or {}).get(\"title\"),\n",
        "            \"source\": (meta or {}).get(\"source_doc\"),\n",
        "            \"chunk\": text,\n",
        "        })\n",
        "        used += len(text)\n",
        "        if len(picked) >= int(top_n):\n",
        "            break\n",
        "    return picked\n",
        "\n",
        "def call_openai_improved(api_key: str, model: str, messages: List[Dict]) -> str:\n",
        "    \"\"\"개선된 OpenAI 호출\"\"\"\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OpenAI API 키가 필요합니다.\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    r = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return r.choices[0].message.content or \"\"\n",
        "\n",
        "_history_deque = deque(maxlen=3)\n",
        "\n",
        "def build_messages_with_history(question: str, context_text: str, citations: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"최근 대화 몇 턴을 포함하도록 메시지를 구성\"\"\"\n",
        "    messages: List[Dict[str, str]] = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    for past_question, past_answer, _ in _history_deque:\n",
        "        if past_question:\n",
        "            messages.append({\"role\": \"user\", \"content\": past_question})\n",
        "        if past_answer:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": past_answer})\n",
        "    user_content = (\n",
        "        f\"질문: {question}\\n\\n컨텍스트:\\n{context_text}\\n\\n\"\n",
        "        \"컨텍스트를 기반으로 답하세요. 답 끝에 '출처' 섹션을 넣어 아래 목록에서 근거를 인용하세요.\\n\"\n",
        "        f\"출처 목록:\\n{citations}\"\n",
        "    )\n",
        "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
        "    return messages\n",
        "\n",
        "def chat_once_improved(question: str, vec_w=DEFAULT_VECTOR_WEIGHT, bm25_w=DEFAULT_BM25_WEIGHT, top_k=DEFAULT_TOP_K, ctx_n=DEFAULT_CONTEXT_TOP_N, max_ctx_chars=DEFAULT_CONTEXT_CHARS, debug=True):\n",
        "    \"\"\"개선된 챗봇 로직\"\"\"\n",
        "    if debug:\n",
        "        print(\"=\"*90)\n",
        "        print(f\"[질문] {question}\")\n",
        "\n",
        "    # 벡터 검색\n",
        "    candidates = vector_search(question, top_k=top_k)\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"[벡터 후보 수] {len(candidates)}\")\n",
        "        print(\"-\"*90)\n",
        "        print(\"[Vector Top 10]\")\n",
        "        for i, (cid, vscore, meta) in enumerate(candidates[:10], 1):\n",
        "            print(f\"  {i:>2}. {vscore:.4f} | {(meta or {}).get('title','N/A')} | {cid}\")\n",
        "\n",
        "    # 개선된 컨텍스트 빌딩\n",
        "    contexts = build_context_improved(question, candidates, vec_w, bm25_w, ctx_n, max_ctx_chars)\n",
        "    \n",
        "    if debug:\n",
        "        print(\"-\"*90)\n",
        "        print(f\"[선택된 컨텍스트 수] {len(contexts)}\")\n",
        "        for i, ctx in enumerate(contexts, 1):\n",
        "            print(f\"  {i:>2}. {ctx['title'] or 'N/A'} | {ctx['id']}\")\n",
        "\n",
        "    # 컨텍스트 텍스트 구성\n",
        "    context_text = \"\\n\\n\".join([f\"[#{i+1}] {c['chunk']}\" for i, c in enumerate(contexts)])\n",
        "    citations = \"\\n\".join(\n",
        "        [f\"- [#{i+1}] {c.get('title') or c.get('source') or c['id']}\" for i, c in enumerate(contexts)]\n",
        "    )\n",
        "\n",
        "    if debug:\n",
        "        print(\"-\"*90)\n",
        "        print(f\"[컨텍스트 길이] {len(context_text)}\")\n",
        "        print(\"=\"*90)\n",
        "\n",
        "    # LLM 호출\n",
        "    messages = build_messages_with_history(question, context_text, citations)\n",
        "\n",
        "    answer = call_openai_improved(\n",
        "        OPENAI_API_KEY,\n",
        "        LLM_MODEL_NAME,\n",
        "        messages,\n",
        "    )\n",
        "\n",
        "    # 히스토리 저장\n",
        "    _history_deque.append((question, answer, \"\"))\n",
        "    \n",
        "    return answer.strip()\n",
        "\n",
        "# ========================================\n",
        "# 중복 실행 방지 체크\n",
        "# ========================================\n",
        "_CHATBOT_PID_FILE = \".chatbot_running.pid\"\n",
        "\n",
        "if os.path.exists(_CHATBOT_PID_FILE):\n",
        "    print(\"⚠️  경고: 챗봇이 이미 실행 중일 수 있습니다!\")\n",
        "    print(\"⚠️  다른 커널에서 실행 중이거나 이전 실행이 완전히 종료되지 않았습니다.\")\n",
        "    print(\"⚠️  해결 방법:\")\n",
        "    print(\"   1. Ctrl+Shift+P → 'Jupyter: Restart Kernel' 실행\")\n",
        "    print(\"   2. 모든 셀을 순차적으로 다시 실행 (1→2→3→4→5)\")\n",
        "    raise RuntimeError(\"중복 실행 방지: 커널을 재시작하세요\")\n",
        "\n",
        "# PID 파일 생성\n",
        "with open(_CHATBOT_PID_FILE, \"w\") as f:\n",
        "    f.write(str(os.getpid()))\n",
        "\n",
        "print(\"✅ 개선된 챗봇 준비 완료\")\n",
        "print(\"🤖 개선된 RAG 챗봇 (exit/quit/종료로 완전 종료)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    _running = True\n",
        "    while _running:\n",
        "        try:\n",
        "            q = input(\"\\n❓ 질문: \").strip()\n",
        "        except (EOFError, KeyboardInterrupt):\n",
        "            print(\"\\n👋 종료합니다.\")\n",
        "            _running = False\n",
        "            break\n",
        "        \n",
        "        if q.lower() in [\"exit\",\"quit\",\"종료\"]:\n",
        "            print(\"👋 종료합니다.\")\n",
        "            _running = False\n",
        "            break\n",
        "        \n",
        "        if not q:\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            print(\"\\n처리 중...⏳\")\n",
        "            out = chat_once_improved(\n",
        "                q, \n",
        "                vec_w=DEFAULT_VECTOR_WEIGHT, \n",
        "                bm25_w=DEFAULT_BM25_WEIGHT, \n",
        "                top_k=DEFAULT_TOP_K, \n",
        "                ctx_n=DEFAULT_CONTEXT_TOP_N, \n",
        "                max_ctx_chars=DEFAULT_CONTEXT_CHARS,\n",
        "                debug=True\n",
        "            )\n",
        "            print(\"\\n💬 답변:\")\n",
        "            print(\"-\"*60)\n",
        "            print(out)\n",
        "            print(\"-\"*60)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ 오류: {e}\")\n",
        "            # 오류가 나도 루프는 계속\n",
        "finally:\n",
        "    # PID 파일 삭제\n",
        "    if os.path.exists(_CHATBOT_PID_FILE):\n",
        "        os.remove(_CHATBOT_PID_FILE)\n",
        "    print(\"\\n✨ 챗봇이 완전히 종료되었습니다.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
