{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Chatbot (Pinecone-only, OpenAI LLM)\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ ì›¹ ì—†ì´ë„ ë¡œì»¬ì—ì„œ ì±—ë´‡ ë™ì‘ì„ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì „ìš© ëŸ°íƒ€ì„ì…ë‹ˆë‹¤.\n",
        "\n",
        "## âš ï¸ ì¤‘ìš”: ì‹¤í–‰ ì „ í•„ìˆ˜ í™•ì¸ì‚¬í•­\n",
        "1. **ì»¤ë„ ì¬ì‹œì‘**: `Ctrl+Shift+P` â†’ \"Jupyter: Restart Kernel\" ì‹¤í–‰\n",
        "2. **ìˆœì°¨ ì‹¤í–‰**: Cell 1 â†’ Cell 2 â†’ Cell 3 â†’ Cell 4 â†’ Cell 5 ìˆœì„œëŒ€ë¡œ ì‹¤í–‰\n",
        "3. **ì¤‘ë³µ ì‹¤í–‰ ê¸ˆì§€**: Cell 5ë¥¼ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•˜ì§€ ë§ˆì„¸ìš” (while ë£¨í”„ê°€ ì¤‘ë³µ ì‹¤í–‰ë¨)\n",
        "\n",
        "ì‹¤í–‰ ìˆœì„œ: 1) í™˜ê²½/ì„¤ì¹˜ â†’ 2) Pinecone ì—°ê²° â†’ 3) ê²€ìƒ‰ í•¨ìˆ˜ â†’ 4) LLM ì±—ë´‡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# í™˜ê²½/ì„¤ì¹˜\n",
        "import sys, platform\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "\n",
        "!{sys.executable} -m pip install -q --upgrade pip\n",
        "!{sys.executable} -m pip install -q \"pinecone>=5.0.0\" sentence-transformers rank-bm25 pyyaml openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pinecone ì—°ê²°\n",
        "import os\n",
        "from pinecone import Pinecone\n",
        "from config import PINECONE_API_KEY, PINECONE_INDEX_NAME\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(PINECONE_INDEX_NAME)\n",
        "print(index.describe_index_stats())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê²€ìƒ‰ í•¨ìˆ˜ (Cell 26 ìš”ì•½)\n",
        "import re, numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from config import EMBEDDING_MODEL_NAME\n",
        "\n",
        "model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=\"cpu\")\n",
        "\n",
        "def simple_tokenize(s: str):\n",
        "    return re.findall(r\"[A-Za-z0-9ê°€-í£]+\", (s or \"\").lower())\n",
        "\n",
        "def vector_search(query: str, top_k: int = 50, meta_filter=None):\n",
        "    q_vec = model.encode([f\"query: {query}\"], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
        "    kwargs = {\"vector\": q_vec.tolist(), \"top_k\": top_k, \"include_values\": False, \"include_metadata\": True}\n",
        "    if meta_filter:\n",
        "        kwargs[\"filter\"] = meta_filter\n",
        "    res = index.query(**kwargs)\n",
        "    return [(m[\"id\"], float(m[\"score\"]), m.get(\"metadata\", {})) for m in res.get(\"matches\", [])]\n",
        "\n",
        "def bm25_over_candidates(query: str, candidates):\n",
        "    ids, docs = [], []\n",
        "    for cid, _, meta in candidates:\n",
        "        text = (meta or {}).get(\"text_content\") or \"\"\n",
        "        if not text:\n",
        "            title = (meta or {}).get(\"title\") or \"\"\n",
        "            keywords = (meta or {}).get(\"keywords\") or \"\"\n",
        "            text = f\"{title}\\n{keywords}\"\n",
        "        ids.append(cid)\n",
        "        docs.append(simple_tokenize(text))\n",
        "    if not docs:\n",
        "        return {}\n",
        "    bm25 = BM25Okapi(docs)\n",
        "    scores = bm25.get_scores(simple_tokenize(query)) if query else np.zeros(len(ids))\n",
        "    max_b = float(np.max(scores)) if len(scores) else 0.0\n",
        "    return {ids[i]: (float(scores[i])/max_b if max_b>0 else 0.0) for i in range(len(ids))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ê°œì„ ëœ LLM ì±—ë´‡\n",
        "from collections import deque\n",
        "from typing import List, Dict, Tuple\n",
        "from openai import OpenAI\n",
        "from config import OPENAI_API_KEY, LLM_MODEL_NAME, DEFAULT_VECTOR_WEIGHT, DEFAULT_BM25_WEIGHT, DEFAULT_TOP_K, DEFAULT_CONTEXT_CHARS, DEFAULT_CONTEXT_TOP_N\n",
        "import os\n",
        "\n",
        "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
        "SYSTEM_PROMPT = (\n",
        "    \"ë„ˆëŠ” RAG ê¸°ë°˜ ë„ìš°ë¯¸ì•¼. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  í™œìš©í•´ì„œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•´.\\n\"\n",
        "    \"ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§í•´.\\n\"\n",
        "    \"ì¶œì²˜ë¥¼ bulletë¡œ í•¨ê»˜ ì œê³µí•´.\"\n",
        "    )\n",
        "\n",
        "def build_context_improved(query: str, candidates: List[Tuple[str, float, Dict]], vec_w: float, bm25_w: float, top_n: int, max_chars: int):\n",
        "    \"\"\"ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ ë¹Œë”©\"\"\"\n",
        "    bm25_scores = bm25_over_candidates(query, candidates)\n",
        "    scored = []\n",
        "    for cid, v_score, meta in candidates:\n",
        "        b_score = bm25_scores.get(cid, 0.0)\n",
        "        combo = vec_w * float(v_score) + bm25_w * float(b_score)\n",
        "        scored.append((combo, cid, meta))\n",
        "    scored.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    picked, used = [], 0\n",
        "    for _, cid, meta in scored[: max(1, int(top_n) * 3)]:\n",
        "        text = (meta or {}).get(\"text_content\") or (meta or {}).get(\"title\") or \"\"\n",
        "        if not text:\n",
        "            continue\n",
        "        if used + len(text) > max_chars:\n",
        "            continue\n",
        "        picked.append({\n",
        "            \"id\": cid,\n",
        "            \"title\": (meta or {}).get(\"title\"),\n",
        "            \"source\": (meta or {}).get(\"source_doc\"),\n",
        "            \"chunk\": text,\n",
        "        })\n",
        "        used += len(text)\n",
        "        if len(picked) >= int(top_n):\n",
        "            break\n",
        "    return picked\n",
        "\n",
        "def call_openai_improved(api_key: str, model: str, messages: List[Dict]) -> str:\n",
        "    \"\"\"ê°œì„ ëœ OpenAI í˜¸ì¶œ\"\"\"\n",
        "    if not api_key:\n",
        "        raise ValueError(\"OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    r = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.2,\n",
        "    )\n",
        "    return r.choices[0].message.content or \"\"\n",
        "\n",
        "_history_deque = deque(maxlen=3)\n",
        "\n",
        "def build_messages_with_history(question: str, context_text: str, citations: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"ìµœê·¼ ëŒ€í™” ëª‡ í„´ì„ í¬í•¨í•˜ë„ë¡ ë©”ì‹œì§€ë¥¼ êµ¬ì„±\"\"\"\n",
        "    messages: List[Dict[str, str]] = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "    for past_question, past_answer, _ in _history_deque:\n",
        "        if past_question:\n",
        "            messages.append({\"role\": \"user\", \"content\": past_question})\n",
        "        if past_answer:\n",
        "            messages.append({\"role\": \"assistant\", \"content\": past_answer})\n",
        "    user_content = (\n",
        "        f\"ì§ˆë¬¸: {question}\\n\\nì»¨í…ìŠ¤íŠ¸:\\n{context_text}\\n\\n\"\n",
        "        \"ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”. ë‹µ ëì— 'ì¶œì²˜' ì„¹ì…˜ì„ ë„£ì–´ ì•„ë˜ ëª©ë¡ì—ì„œ ê·¼ê±°ë¥¼ ì¸ìš©í•˜ì„¸ìš”.\\n\"\n",
        "        f\"ì¶œì²˜ ëª©ë¡:\\n{citations}\"\n",
        "    )\n",
        "    messages.append({\"role\": \"user\", \"content\": user_content})\n",
        "    return messages\n",
        "\n",
        "def chat_once_improved(question: str, vec_w=DEFAULT_VECTOR_WEIGHT, bm25_w=DEFAULT_BM25_WEIGHT, top_k=DEFAULT_TOP_K, ctx_n=DEFAULT_CONTEXT_TOP_N, max_ctx_chars=DEFAULT_CONTEXT_CHARS, debug=True):\n",
        "    \"\"\"ê°œì„ ëœ ì±—ë´‡ ë¡œì§\"\"\"\n",
        "    if debug:\n",
        "        print(\"=\"*90)\n",
        "        print(f\"[ì§ˆë¬¸] {question}\")\n",
        "\n",
        "    # ë²¡í„° ê²€ìƒ‰\n",
        "    candidates = vector_search(question, top_k=top_k)\n",
        "    \n",
        "    if debug:\n",
        "        print(f\"[ë²¡í„° í›„ë³´ ìˆ˜] {len(candidates)}\")\n",
        "        print(\"-\"*90)\n",
        "        print(\"[Vector Top 10]\")\n",
        "        for i, (cid, vscore, meta) in enumerate(candidates[:10], 1):\n",
        "            print(f\"  {i:>2}. {vscore:.4f} | {(meta or {}).get('title','N/A')} | {cid}\")\n",
        "\n",
        "    # ê°œì„ ëœ ì»¨í…ìŠ¤íŠ¸ ë¹Œë”©\n",
        "    contexts = build_context_improved(question, candidates, vec_w, bm25_w, ctx_n, max_ctx_chars)\n",
        "    \n",
        "    if debug:\n",
        "        print(\"-\"*90)\n",
        "        print(f\"[ì„ íƒëœ ì»¨í…ìŠ¤íŠ¸ ìˆ˜] {len(contexts)}\")\n",
        "        for i, ctx in enumerate(contexts, 1):\n",
        "            print(f\"  {i:>2}. {ctx['title'] or 'N/A'} | {ctx['id']}\")\n",
        "\n",
        "    # ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±\n",
        "    context_text = \"\\n\\n\".join([f\"[#{i+1}] {c['chunk']}\" for i, c in enumerate(contexts)])\n",
        "    citations = \"\\n\".join(\n",
        "        [f\"- [#{i+1}] {c.get('title') or c.get('source') or c['id']}\" for i, c in enumerate(contexts)]\n",
        "    )\n",
        "\n",
        "    if debug:\n",
        "        print(\"-\"*90)\n",
        "        print(f\"[ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´] {len(context_text)}\")\n",
        "        print(\"=\"*90)\n",
        "\n",
        "    # LLM í˜¸ì¶œ\n",
        "    messages = build_messages_with_history(question, context_text, citations)\n",
        "\n",
        "    answer = call_openai_improved(\n",
        "        OPENAI_API_KEY,\n",
        "        LLM_MODEL_NAME,\n",
        "        messages,\n",
        "    )\n",
        "\n",
        "    # íˆìŠ¤í† ë¦¬ ì €ì¥\n",
        "    _history_deque.append((question, answer, \"\"))\n",
        "    \n",
        "    return answer.strip()\n",
        "\n",
        "# ========================================\n",
        "# ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ ì²´í¬\n",
        "# ========================================\n",
        "_CHATBOT_PID_FILE = \".chatbot_running.pid\"\n",
        "\n",
        "if os.path.exists(_CHATBOT_PID_FILE):\n",
        "    print(\"âš ï¸  ê²½ê³ : ì±—ë´‡ì´ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤!\")\n",
        "    print(\"âš ï¸  ë‹¤ë¥¸ ì»¤ë„ì—ì„œ ì‹¤í–‰ ì¤‘ì´ê±°ë‚˜ ì´ì „ ì‹¤í–‰ì´ ì™„ì „íˆ ì¢…ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"âš ï¸  í•´ê²° ë°©ë²•:\")\n",
        "    print(\"   1. Ctrl+Shift+P â†’ 'Jupyter: Restart Kernel' ì‹¤í–‰\")\n",
        "    print(\"   2. ëª¨ë“  ì…€ì„ ìˆœì°¨ì ìœ¼ë¡œ ë‹¤ì‹œ ì‹¤í–‰ (1â†’2â†’3â†’4â†’5)\")\n",
        "    raise RuntimeError(\"ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€: ì»¤ë„ì„ ì¬ì‹œì‘í•˜ì„¸ìš”\")\n",
        "\n",
        "# PID íŒŒì¼ ìƒì„±\n",
        "with open(_CHATBOT_PID_FILE, \"w\") as f:\n",
        "    f.write(str(os.getpid()))\n",
        "\n",
        "print(\"âœ… ê°œì„ ëœ ì±—ë´‡ ì¤€ë¹„ ì™„ë£Œ\")\n",
        "print(\"ğŸ¤– ê°œì„ ëœ RAG ì±—ë´‡ (exit/quit/ì¢…ë£Œë¡œ ì™„ì „ ì¢…ë£Œ)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    _running = True\n",
        "    while _running:\n",
        "        try:\n",
        "            q = input(\"\\nâ“ ì§ˆë¬¸: \").strip()\n",
        "        except (EOFError, KeyboardInterrupt):\n",
        "            print(\"\\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "            _running = False\n",
        "            break\n",
        "        \n",
        "        if q.lower() in [\"exit\",\"quit\",\"ì¢…ë£Œ\"]:\n",
        "            print(\"ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "            _running = False\n",
        "            break\n",
        "        \n",
        "        if not q:\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            print(\"\\nì²˜ë¦¬ ì¤‘...â³\")\n",
        "            out = chat_once_improved(\n",
        "                q, \n",
        "                vec_w=DEFAULT_VECTOR_WEIGHT, \n",
        "                bm25_w=DEFAULT_BM25_WEIGHT, \n",
        "                top_k=DEFAULT_TOP_K, \n",
        "                ctx_n=DEFAULT_CONTEXT_TOP_N, \n",
        "                max_ctx_chars=DEFAULT_CONTEXT_CHARS,\n",
        "                debug=True\n",
        "            )\n",
        "            print(\"\\nğŸ’¬ ë‹µë³€:\")\n",
        "            print(\"-\"*60)\n",
        "            print(out)\n",
        "            print(\"-\"*60)\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ ì˜¤ë¥˜: {e}\")\n",
        "            # ì˜¤ë¥˜ê°€ ë‚˜ë„ ë£¨í”„ëŠ” ê³„ì†\n",
        "finally:\n",
        "    # PID íŒŒì¼ ì‚­ì œ\n",
        "    if os.path.exists(_CHATBOT_PID_FILE):\n",
        "        os.remove(_CHATBOT_PID_FILE)\n",
        "    print(\"\\nâœ¨ ì±—ë´‡ì´ ì™„ì „íˆ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
