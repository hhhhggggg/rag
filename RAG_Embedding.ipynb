{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Embedding Pipeline (Pinecone + E5)\n",
        "\n",
        "Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÄ Î¨∏ÏÑú ÏóÖÏÑúÌä∏Î•º ÏúÑÌïú Ï†ÑÏö© ÌååÏù¥ÌîÑÎùºÏù∏ÏûÖÎãàÎã§.\n",
        "\n",
        "Ïã§Ìñâ ÏàúÏÑú: 1) ÌôòÍ≤Ω/ÏÑ§Ïπò ‚Üí 2) Pinecone Ïó∞Í≤∞ ‚Üí 3) Î¨∏ÏÑú ÏàòÏßë/ÌååÏã±/Ï≤≠ÌÇπ ‚Üí 4) Î™®Îç∏ Î°úÎìú/ÏóÖÏÑúÌä∏ ‚Üí (ÏÑ†ÌÉù) Ïù∏Îç±Ïä§ Ï¥àÍ∏∞Ìôî\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]\n",
            "Platform: Windows-10-10.0.26100-SP0\n",
            "Torch: 2.8.0+cpu | CUDA: False\n",
            "Transformers: 4.56.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~umpy (c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "# ÌôòÍ≤Ω/ÏÑ§Ïπò (Cell 2‚Äì4)\n",
        "import sys, platform\n",
        "print('Python:', sys.version)\n",
        "print('Platform:', platform.platform())\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print('Torch:', torch.__version__, '| CUDA:', torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    print('Torch error:', e)\n",
        "\n",
        "try:\n",
        "    import transformers\n",
        "    print('Transformers:', transformers.__version__)\n",
        "except Exception as e:\n",
        "    print('Transformers error:', e)\n",
        "\n",
        "# ÏÑ§Ïπò (Ï°∞Ïö©Ìûà)\n",
        "!{sys.executable} -m pip install -q --upgrade pip\n",
        "!{sys.executable} -m pip install -q \"pinecone>=5.0.0\" sentence-transformers python-slugify rank-bm25 pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ïù¥ÎØ∏ Ïù∏Îç±Ïä§ ÏûàÏùå\n",
            "{'dimension': 1024,\n",
            " 'index_fullness': 0.0,\n",
            " 'metric': 'cosine',\n",
            " 'namespaces': {'': {'vector_count': 1082}},\n",
            " 'total_vector_count': 1082,\n",
            " 'vector_type': 'dense'}\n"
          ]
        }
      ],
      "source": [
        "# Pinecone Ïó∞Í≤∞/Ïù∏Îç±Ïä§ (Cell 6, 9)\n",
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from config import PINECONE_API_KEY, PINECONE_INDEX_NAME, PINECONE_DIMENSION, PINECONE_METRIC, PINECONE_CLOUD, PINECONE_REGION\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index_name = PINECONE_INDEX_NAME\n",
        "\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=PINECONE_DIMENSION,\n",
        "        metric=PINECONE_METRIC,\n",
        "        spec=ServerlessSpec(cloud=PINECONE_CLOUD, region=PINECONE_REGION)\n",
        "    )\n",
        "    print(\"Ïù∏Îç±Ïä§ ÏÉùÏÑ± ÏôÑÎ£å\")\n",
        "else:\n",
        "    print(\"Ïù¥ÎØ∏ Ïù∏Îç±Ïä§ ÏûàÏùå\")\n",
        "\n",
        "index = pc.Index(index_name)\n",
        "print(index.describe_index_stats())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Î¨∏ÏÑú ÏàòÏßë/ÌååÏã±/Ï≤≠ÌÇπ (Cell 12, 14, 18)\n",
        "import os, re, json, yaml\n",
        "from slugify import slugify\n",
        "from config import DOCUMENT_PATHS, CHUNK_SIZE, CHUNK_OVERLAP\n",
        "\n",
        "def find_md_files_direct(folder_path):\n",
        "    md_files = []\n",
        "    if os.path.exists(folder_path):\n",
        "        for file in os.listdir(folder_path):\n",
        "            if file.endswith(\".md\"):\n",
        "                md_files.append(os.path.join(folder_path, file))\n",
        "    return md_files\n",
        "\n",
        "# config.pyÏóêÏÑú Í≤ΩÎ°ú Í∞ÄÏ†∏Ïò§Í∏∞\n",
        "folder_path1 = DOCUMENT_PATHS[\"products\"]\n",
        "folder_path2 = DOCUMENT_PATHS[\"company\"]\n",
        "md_files = find_md_files_direct(folder_path1) + find_md_files_direct(folder_path2)\n",
        "print(\"Ï†ÑÏ≤¥ ÎßàÌÅ¨Îã§Ïö¥ ÌååÏùº Ïàò:\", len(md_files))\n",
        "\n",
        "fm_re = re.compile(r\"^---\\s*\\n(.*?)\\n---\\s*\\n(.*)\", re.DOTALL)\n",
        "\n",
        "def parse_document_unified(text, filename):\n",
        "    m = fm_re.match(text)\n",
        "    if m:\n",
        "        fm_raw, body = m.groups()\n",
        "        try:\n",
        "            fm_dict = yaml.safe_load(fm_raw)\n",
        "            if fm_dict:\n",
        "                return fm_dict, body\n",
        "        except yaml.YAMLError:\n",
        "            pass\n",
        "    name_without_ext = os.path.splitext(filename)[0]\n",
        "    title = name_without_ext.replace('_', ' ')\n",
        "    fm_dict = {\"title\": title, \"category1\": \"Í∏∞ÌÉÄ\", \"category2\": \"\", \"category3\": \"\", \"category4\": \"\", \"keywords\": title}\n",
        "    return fm_dict, text\n",
        "\n",
        "def chunk_text(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
        "    chunks, start = [], 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "docs = []\n",
        "for file_path in md_files:\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        t = f.read()\n",
        "    fm, body = parse_document_unified(t, os.path.basename(file_path))\n",
        "    docs.append({\"id\": os.path.basename(file_path), \"front_matter\": fm, \"body\": body})\n",
        "\n",
        "print(\"Î∂àÎü¨Ïò® Î¨∏ÏÑú Ïàò:\", len(docs))\n",
        "\n",
        "chunked_docs_meta = []\n",
        "for doc in docs:\n",
        "    fm = doc.get(\"front_matter\") or {}\n",
        "    base_meta = {\n",
        "        \"source_doc\": doc[\"id\"],\n",
        "        \"title\": fm.get(\"title\", \"\"),\n",
        "        \"category1\": fm.get(\"category1\", \"\"),\n",
        "        \"category2\": fm.get(\"category2\", \"\"),\n",
        "        \"category3\": fm.get(\"category3\", \"\"),\n",
        "        \"category4\": fm.get(\"category4\", \"\"),\n",
        "        \"keywords\": fm.get(\"keywords\", \"\"),\n",
        "    }\n",
        "    clean_name = slugify(os.path.splitext(doc[\"id\"])[0], separator='_') or \"doc\"\n",
        "    # front-matterÎèÑ ÌïòÎÇòÏùò Î†àÏΩîÎìúÎ°ú Ï†ÄÏû•\n",
        "    if fm:\n",
        "        fm_json = json.dumps(fm, ensure_ascii=False)\n",
        "        chunked_docs_meta.append({\n",
        "            \"id\": f\"{clean_name}-front\",\n",
        "            \"text\": f\"passage: {fm_json}\",\n",
        "            \"metadata\": {**base_meta, \"kind\": \"front\", \"text_content\": fm_json[:40000]}\n",
        "        })\n",
        "    for i, chunk in enumerate(chunk_text(doc[\"body\"], CHUNK_SIZE, CHUNK_OVERLAP)):\n",
        "        chunked_docs_meta.append({\n",
        "            \"id\": f\"{clean_name}-chunk-{i}\",\n",
        "            \"text\": f\"passage: {chunk}\",\n",
        "            \"metadata\": {**base_meta, \"kind\": \"chunk\", \"text_content\": chunk[:40000]}\n",
        "        })\n",
        "\n",
        "print(\"Ï≤≠ÌÅ¨ Ïàò:\", len(chunked_docs_meta))\n",
        "print(\"ÏòàÏãú:\", chunked_docs_meta[0][\"id\"] if chunked_docs_meta else None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Î™®Îç∏ Î°úÎìú/ÏóÖÏÑúÌä∏ (Cell 21, 22)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from config import EMBEDDING_MODEL_NAME\n",
        "\n",
        "model_name = EMBEDDING_MODEL_NAME\n",
        "model = SentenceTransformer(model_name, device=\"cpu\")\n",
        "print(\"Î™®Îç∏ Î°úÎìú ÏôÑÎ£å:\", model_name)\n",
        "\n",
        "# ÏóÖÏÑúÌä∏\n",
        "records = []\n",
        "texts = [d[\"text\"] for d in chunked_docs_meta]\n",
        "embs = model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "for i, d in enumerate(chunked_docs_meta):\n",
        "    records.append({\n",
        "        \"id\": d[\"id\"],\n",
        "        \"values\": embs[i].tolist(),\n",
        "        \"metadata\": d[\"metadata\"]\n",
        "    })\n",
        "\n",
        "print(\"ÏóÖÏÑúÌä∏ Ï§ÄÎπÑ:\", len(records))\n",
        "\n",
        "batch_size = 100\n",
        "for s in range(0, len(records), batch_size):\n",
        "    batch = records[s:s+batch_size]\n",
        "    index.upsert(vectors=batch)\n",
        "print(\"ÏóÖÏÑúÌä∏ ÏôÑÎ£å\")\n",
        "print(index.describe_index_stats())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (ÏÑ†ÌÉù) Ïù∏Îç±Ïä§ Ï¥àÍ∏∞Ìôî (Cell 15, 16) - ÏúÑÌóò\n",
        "print(\"üóëÔ∏è Ïù∏Îç±Ïä§ Ï†ÑÏ≤¥ ÏÇ≠Ï†ú (Ïã§Ìñâ Ï†Ñ Ï£ºÏùò)\")\n",
        "# ÏïÑÎûò Îëê Ï§ÑÏùÑ Ï£ºÏÑù Ìï¥Ï†ú ÌõÑ Ïã§Ìñâ\n",
        "# stats = index.describe_index_stats(); print(stats)\n",
        "# index.delete(delete_all=True); print(index.describe_index_stats())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
