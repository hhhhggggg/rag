import streamlit as st
import re
import numpy as np
from collections import deque
from typing import List, Dict, Tuple
from pinecone import Pinecone
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from openai import OpenAI

# =====================================
# ğŸ” Secrets ì„¤ì • (Streamlit Cloud)
# =====================================
PINECONE_API_KEY = st.secrets["PINECONE_API_KEY"]
PINECONE_INDEX_NAME = st.secrets["PINECONE_INDEX_NAME"]
EMBEDDING_MODEL_NAME = st.secrets["EMBEDDING_MODEL_NAME"]
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
LLM_MODEL_NAME = st.secrets["LLM_MODEL_NAME"]

DEFAULT_VECTOR_WEIGHT = float(st.secrets.get("DEFAULT_VECTOR_WEIGHT", 0.7))
DEFAULT_BM25_WEIGHT = float(st.secrets.get("DEFAULT_BM25_WEIGHT", 0.3))
DEFAULT_TOP_K = int(st.secrets.get("DEFAULT_TOP_K", 50))
DEFAULT_CONTEXT_CHARS = int(st.secrets.get("DEFAULT_CONTEXT_CHARS", 3000))
DEFAULT_CONTEXT_TOP_N = int(st.secrets.get("DEFAULT_CONTEXT_TOP_N", 5))

# =====================================
# âš™ï¸ Streamlit ê¸°ë³¸ ì„¤ì •
# =====================================
st.set_page_config(page_title="RAG Chatbot", page_icon="ğŸ¤–", layout="wide")
st.title("ğŸ’¬ RAG Chatbot (Pinecone + OpenAI)")
st.caption("Sentence-Transformer + BM25 + OpenAI ê¸°ë°˜ì˜ RAG ì±—ë´‡")

# =====================================
# ğŸ”— Pinecone ì—°ê²°
# =====================================
@st.cache_resource
def load_pinecone_index():
    pc = Pinecone(api_key=PINECONE_API_KEY)
    return pc.Index(PINECONE_INDEX_NAME)

try:
    index = load_pinecone_index()
    st.success(f"Pinecone Index ì—°ê²°ë¨ âœ… ({PINECONE_INDEX_NAME})")
except Exception as e:
    st.error(f"Pinecone ì—°ê²° ì‹¤íŒ¨ âŒ: {e}")
    st.stop()

# =====================================
# ğŸ”¤ SentenceTransformer ì„ë² ë”© ëª¨ë¸
# =====================================
@st.cache_resource
def load_embedder():
    return SentenceTransformer(EMBEDDING_MODEL_NAME, device="cpu")

model = load_embedder()

# =====================================
# ğŸ” ê²€ìƒ‰ í•¨ìˆ˜
# =====================================
def simple_tokenize(s: str):
    return re.findall(r"[A-Za-z0-9ê°€-í£]+", (s or "").lower())

def vector_search(query: str, top_k: int = 50, meta_filter=None):
    q_vec = model.encode([f"query: {query}"], convert_to_numpy=True, normalize_embeddings=True)[0]
    kwargs = {
        "vector": q_vec.tolist(),
        "top_k": top_k,
        "include_values": False,
        "include_metadata": True,
    }
    if meta_filter:
        kwargs["filter"] = meta_filter
    res = index.query(**kwargs)
    return [(m["id"], float(m["score"]), m.get("metadata", {})) for m in res.get("matches", [])]

def bm25_over_candidates(query: str, candidates):
    ids, docs = [], []
    for cid, _, meta in candidates:
        text = (meta or {}).get("text_content") or ""
        if not text:
            title = (meta or {}).get("title") or ""
            keywords = (meta or {}).get("keywords") or ""
            text = f"{title}\n{keywords}"
        ids.append(cid)
        docs.append(simple_tokenize(text))
    if not docs:
        return {}
    bm25 = BM25Okapi(docs)
    scores = bm25.get_scores(simple_tokenize(query)) if query else np.zeros(len(ids))
    max_b = float(np.max(scores)) if len(scores) else 0.0
    return {ids[i]: (float(scores[i]) / max_b if max_b > 0 else 0.0) for i in range(len(ids))}

# =====================================
# ğŸ§  RAG + LLM ë¡œì§
# =====================================
openai_client = OpenAI(api_key=OPENAI_API_KEY)

SYSTEM_PROMPT = (
    "ë„ˆëŠ” RAG ê¸°ë°˜ ë„ìš°ë¯¸ì•¼. ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  í™œìš©í•´ì„œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•´.\n"
    "ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§í•´.\n"
    "ë‹µë³€ ëì—ëŠ” ì¶œì²˜ë¥¼ bullet í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•´."
)

def build_context_improved(query, candidates, vec_w, bm25_w, top_n, max_chars):
    bm25_scores = bm25_over_candidates(query, candidates)
    scored = []
    for cid, v_score, meta in candidates:
        b_score = bm25_scores.get(cid, 0.0)
        combo = vec_w * float(v_score) + bm25_w * float(b_score)
        scored.append((combo, cid, meta))
    scored.sort(reverse=True, key=lambda x: x[0])

    picked, used = [], 0
    for _, cid, meta in scored[: max(1, int(top_n) * 3)]:
        text = (meta or {}).get("text_content") or (meta or {}).get("title") or ""
        if not text:
            continue
        if used + len(text) > max_chars:
            continue
        picked.append({
            "id": cid,
            "title": (meta or {}).get("title"),
            "source": (meta or {}).get("source_doc"),
            "chunk": text,
        })
        used += len(text)
        if len(picked) >= int(top_n):
            break
    return picked

def call_openai_improved(api_key: str, model: str, messages: List[Dict]) -> str:
    client = OpenAI(api_key=api_key)
    r = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0.2,
    )
    return r.choices[0].message.content or ""

# =====================================
# ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬
# =====================================
if "history" not in st.session_state:
    st.session_state.history = deque(maxlen=3)

def chat_once(question: str):
    candidates = vector_search(question, top_k=DEFAULT_TOP_K)
    contexts = build_context_improved(
        question,
        candidates,
        DEFAULT_VECTOR_WEIGHT,
        DEFAULT_BM25_WEIGHT,
        DEFAULT_CONTEXT_TOP_N,
        DEFAULT_CONTEXT_CHARS,
    )
    context_text = "\n\n".join([f"[#{i+1}] {c['chunk']}" for i, c in enumerate(contexts)])
    citations = "\n".join(
        [f"- [#{i+1}] {c.get('title') or c.get('source') or c['id']}" for i, c in enumerate(contexts)]
    )

    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    for past_q, past_a, _ in st.session_state.history:
        if past_q:
            messages.append({"role": "user", "content": past_q})
        if past_a:
            messages.append({"role": "assistant", "content": past_a})

    user_msg = (
        f"ì§ˆë¬¸: {question}\n\nì»¨í…ìŠ¤íŠ¸:\n{context_text}\n\n"
        "ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”. ë‹µ ëì— 'ì¶œì²˜' ì„¹ì…˜ì„ ë„£ì–´ ì•„ë˜ ëª©ë¡ì—ì„œ ê·¼ê±°ë¥¼ ì¸ìš©í•˜ì„¸ìš”.\n"
        f"ì¶œì²˜ ëª©ë¡:\n{citations}"
    )
    messages.append({"role": "user", "content": user_msg})

    answer = call_openai_improved(OPENAI_API_KEY, LLM_MODEL_NAME, messages)
    st.session_state.history.append((question, answer, ""))

    return answer.strip(), contexts

# =====================================
# ğŸ§­ Streamlit UI
# =====================================
st.subheader("ğŸ’­ ëŒ€í™”í•˜ê¸°")

query = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", height=100, placeholder="ì˜ˆ: Pinecone ì¸ë±ìŠ¤ëŠ” ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ìš”?")

if st.button("ì§ˆë¬¸í•˜ê¸°", type="primary"):
    if not query.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘... â³"):
            try:
                answer, ctx = chat_once(query)
                st.markdown("### ğŸ’¬ ë‹µë³€")
                st.write(answer)

                with st.expander("ğŸ” ì‚¬ìš©ëœ ì»¨í…ìŠ¤íŠ¸ ë³´ê¸°"):
                    for i, c in enumerate(ctx, 1):
                        st.markdown(f"**#{i}. {c['title'] or 'ì œëª© ì—†ìŒ'}**")
                        st.text(c['chunk'][:400] + ("..." if len(c['chunk']) > 400 else ""))
            except Exception as e:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")

# =====================================
# âš™ï¸ ì‚¬ì´ë“œë°”
# =====================================
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    st.info("Secrets.tomlì— API í‚¤ë¥¼ ì €ì¥í•˜ë©´ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
    st.write("í˜„ì¬ Pinecone ì¸ë±ìŠ¤:", PINECONE_INDEX_NAME)
    if st.button("ğŸ’£ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state.history.clear()
        st.success("ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
